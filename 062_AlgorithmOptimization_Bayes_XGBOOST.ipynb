{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bayesian_optimization(dataset, function, parameters):\n",
    "#    X_train, y_train, X_test, y_test = dataset\n",
    "#    n_iterations = 5\n",
    "#    gp_params = {\"alpha\": 1e-4}\n",
    "\n",
    "#    BO = BayesianOptimization(function, parameters)\n",
    "#    BO.maximize(n_iter=n_iterations, **gp_params)\n",
    "\n",
    "#    return BO.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rfc_optimization(cv_splits):\n",
    "#     def function(n_estimators, max_depth, min_samples_split):\n",
    "#         return cross_val_score(\n",
    "#                RandomForestClassifier(\n",
    "#                    n_estimators=int(max(n_estimators,0)),                                                               \n",
    "#                    max_depth=int(max(max_depth,1)),\n",
    "#                    min_samples_split=int(max(min_samples_split,2)), \n",
    "#                    n_jobs=-1, \n",
    "#                    random_state=42,   \n",
    "#                    class_weight=\"balanced\"),  \n",
    "#                X=X_train, \n",
    "#                y=y_train, \n",
    "#                cv=cv_splits,\n",
    "#                scoring=\"roc_auc\",\n",
    "#                n_jobs=-1).mean()\n",
    "\n",
    "#     parameters = {\"n_estimators\": (10, 1000),\n",
    "#                   \"max_depth\": (1, 150),\n",
    "#                   \"min_samples_split\": (2, 10)}\n",
    "    \n",
    "#     return function, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def xgb_optimization(cv_splits, eval_set):\n",
    "#     def function(eta, gamma, max_depth):\n",
    "#             return cross_val_score(\n",
    "#                    xgb.XGBClassifier(\n",
    "#                        objective=\"binary:logistic\",\n",
    "#                        learning_rate=max(eta, 0),\n",
    "#                        gamma=max(gamma, 0),\n",
    "#                        max_depth=int(max_depth),                                               \n",
    "#                        seed=42,\n",
    "#                        nthread=-1,\n",
    "#                        scale_pos_weight = len(y_train[y_train == 0])/\n",
    "#                                           len(y_train[y_train == 1])),  \n",
    "#                    X=X_train, \n",
    "#                    y=y_train, \n",
    "#                    cv=cv_splits,\n",
    "#                    scoring=\"roc_auc\",\n",
    "#                    fit_params={\n",
    "#                         \"early_stopping_rounds\": 10, \n",
    "#                         \"eval_metric\": \"auc\", \n",
    "#                         \"eval_set\": eval_set},\n",
    "#                    n_jobs=-1).mean()\n",
    "\n",
    "#     parameters = {\"eta\": (0.001, 0.4),\n",
    "#                   \"gamma\": (0, 20),\n",
    "#                   \"max_depth\": (1, 2000)}\n",
    "    \n",
    "#     return function, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(X_train, y_train, X_test, y_test, function, parameters):\n",
    "#     dataset = (X_train, y_train, X_test, y_test)\n",
    "#     cv_splits = 4\n",
    "    \n",
    "#     best_solution = bayesian_optimization(dataset, function, parameters)      \n",
    "#     params = best_solution[\"params\"]\n",
    "\n",
    "#     model = RandomForestClassifier(\n",
    "#              n_estimators=int(max(params[\"n_estimators\"], 0)),\n",
    "#              max_depth=int(max(params[\"max_depth\"], 1)),\n",
    "#              min_samples_split=int(max(params[\"min_samples_split\"], 2)), \n",
    "#              n_jobs=-1, \n",
    "#              random_state=42,   \n",
    "#              class_weight=\"balanced\")\n",
    "\n",
    "#     model.fit(X_train, y_train)\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.cross_validation import cross_val_score, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import log_loss, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "import contextlib\n",
    "\n",
    "\n",
    "# def read_dataset(fname):\n",
    "#     data = pd.read_csv(fname, encoding=\"utf-8\")\n",
    "#     # drop掉无用数据\n",
    "#     data.drop(['obs_time', 'patient_id', 'dataset_name'], axis=1, inplace=True)\n",
    "#     return data\n",
    "\n",
    "# fileName = open(r\"/Users/tuyu/000000毕业设计/Flight_Delay/Train_Feature/final_feature.csv\")\n",
    "# data_file = pd.read_csv(fileName)\n",
    "feature = pd.read_table('/Users/tuyu/000000毕业设计/Flight_Delay/Train_Feature/final_feature.csv',sep=',',encoding='gb2312')\n",
    "del(feature['本时段离港平均延误时长'])\n",
    "del(feature['上一时段离港平均延误时长'])\n",
    "del(feature['航班编号'])\n",
    "# 1.前序延误 = 前序航班 实际到达时间 - 计划到达时间\n",
    "# 2.起飞间隔 = 当前航班的计划起飞时间 - 前序航班实际到达时间\n",
    "feature['起飞间隔'] = feature['起飞间隔'].fillna(0)\n",
    "feature['前序延误'] = feature['前序延误'].fillna(0)\n",
    "feature['出发机场天气得分'] = feature['出发机场天气得分'].interpolate()\n",
    "feature['到达机场天气得分'] = feature['到达机场天气得分'].interpolate()\n",
    "feature['平均延误时间'] = feature['平均延误时间'].interpolate()\n",
    "feature['最大延误时间'] = feature['最大延误时间'].interpolate()\n",
    "feature['延误时间中位数'] = feature['延误时间中位数'].interpolate()\n",
    "feature['延误时间标准差'] = feature['延误时间标准差'].interpolate()\n",
    "\n",
    "feature['本时段计划离港航班数'] = feature['本时段计划离港航班数'].fillna(feature['本时段计划离港航班数'].mean())\n",
    "feature['本时段实际离港航班数'] = feature['本时段实际离港航班数'].fillna(feature['本时段实际离港航班数'].mean())\n",
    "feature['本时段进港延误时长'] = feature['本时段进港延误时长'].fillna(feature['本时段进港延误时长'].mean())\n",
    "feature['本时段实际进港延误航班数'] = feature['本时段实际进港延误航班数'].fillna(feature['本时段实际进港延误航班数'].mean())\n",
    "feature['本时段离港延误时长'] = feature['本时段离港延误时长'].fillna(feature['本时段离港延误时长'].mean())\n",
    "feature['本时段离港延误率'] = feature['本时段离港延误率'].fillna(feature['本时段离港延误率'].mean())\n",
    "feature['上一时段进港延误时长'] = feature['上一时段进港延误时长'].fillna(feature['上一时段进港延误时长'].mean())\n",
    "feature['上一时段离港延误时长'] = feature['上一时段离港延误时长'].fillna(feature['上一时段离港延误时长'].mean())\n",
    "feature['上一时段实际进港航班数'] = feature['上一时段实际进港航班数'].fillna(feature['上一时段实际进港航班数'].mean())\n",
    "feature['上一时段进港延误率'] = feature['上一时段进港延误率'].fillna(feature['上一时段进港延误率'].mean())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature0 = feature[feature['标签'] == 0]\n",
    "feature0new = feature0.sample(frac=0.1, replace=False, random_state=None,axis=0)\n",
    "feature1 = feature[feature['标签'] == 1]\n",
    "\n",
    "\n",
    "feature0Bayes = feature0new.sample(frac=0.08, replace=False, random_state=None,axis=0)\n",
    "feature1Bayes = feature1.sample(frac=0.06, replace=False, random_state=None,axis=0)\n",
    "\n",
    "featureBayes = pd.concat([feature0Bayes,feature1Bayes],axis=0, ignore_index=True)\n",
    "\n",
    "train = featureBayes\n",
    "# # train = read_dataset('190624_data.csv')\n",
    "y = train['标签'].values\n",
    "X = train.drop(['标签'], axis=1).values\n",
    "\n",
    "# # y = y + 0  # 将布尔值替换成01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "|   iter    |  target   | colsam... |   gamma   | learni... | max_de... | max_depth | min_ch... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      " Stopped after 66 iterations with train-auc = 0.977102 val-auc = 0.949101 ( diff = 0.028001 ) train-gini = 0.954205 val-gini = 0.898202\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8982  \u001b[0m | \u001b[0m 0.4702  \u001b[0m | \u001b[0m 6.089   \u001b[0m | \u001b[0m 0.2584  \u001b[0m | \u001b[0m 1.574   \u001b[0m | \u001b[0m 10.98   \u001b[0m | \u001b[0m 1.525   \u001b[0m | \u001b[0m 0.7647  \u001b[0m |\n",
      " Stopped after 207 iterations with train-auc = 0.962166 val-auc = 0.946994 ( diff = 0.015171 ) train-gini = 0.924331 val-gini = 0.893988\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.894   \u001b[0m | \u001b[0m 0.5258  \u001b[0m | \u001b[0m 9.254   \u001b[0m | \u001b[0m 0.2114  \u001b[0m | \u001b[0m 1.033   \u001b[0m | \u001b[0m 8.538   \u001b[0m | \u001b[0m 17.31   \u001b[0m | \u001b[0m 0.4496  \u001b[0m |\n",
      " Stopped after 1765 iterations with train-auc = 0.963177 val-auc = 0.949939 ( diff = 0.013237 ) train-gini = 0.926353 val-gini = 0.899878\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.8999  \u001b[0m | \u001b[95m 0.7527  \u001b[0m | \u001b[95m 9.431   \u001b[0m | \u001b[95m 0.03966 \u001b[0m | \u001b[95m 0.5509  \u001b[0m | \u001b[95m 5.547   \u001b[0m | \u001b[95m 16.79   \u001b[0m | \u001b[95m 0.8193  \u001b[0m |\n",
      " Stopped after 127 iterations with train-auc = 0.971684 val-auc = 0.949449 ( diff = 0.022235 ) train-gini = 0.943368 val-gini = 0.898898\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.8989  \u001b[0m | \u001b[0m 0.9232  \u001b[0m | \u001b[0m 0.3865  \u001b[0m | \u001b[0m 0.09085 \u001b[0m | \u001b[0m 0.1696  \u001b[0m | \u001b[0m 9.19    \u001b[0m | \u001b[0m 13.65   \u001b[0m | \u001b[0m 0.4696  \u001b[0m |\n",
      " Stopped after 41 iterations with train-auc = 0.964739 val-auc = 0.945299 ( diff = 0.019440 ) train-gini = 0.929478 val-gini = 0.890599\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.8906  \u001b[0m | \u001b[0m 0.4423  \u001b[0m | \u001b[0m 9.592   \u001b[0m | \u001b[0m 0.447   \u001b[0m | \u001b[0m 7.0     \u001b[0m | \u001b[0m 11.81   \u001b[0m | \u001b[0m 3.136   \u001b[0m | \u001b[0m 0.5971  \u001b[0m |\n",
      " Stopped after 9 iterations with train-auc = 0.947145 val-auc = 0.937133 ( diff = 0.010012 ) train-gini = 0.894290 val-gini = 0.874266\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.8743  \u001b[0m | \u001b[0m 0.4199  \u001b[0m | \u001b[0m 8.209   \u001b[0m | \u001b[0m 0.7981  \u001b[0m | \u001b[0m 2.197   \u001b[0m | \u001b[0m 7.478   \u001b[0m | \u001b[0m 1.16    \u001b[0m | \u001b[0m 0.5379  \u001b[0m |\n",
      " Stopped after 162 iterations with train-auc = 0.977620 val-auc = 0.950025 ( diff = 0.027595 ) train-gini = 0.955239 val-gini = 0.900050\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m 0.9     \u001b[0m | \u001b[95m 0.589   \u001b[0m | \u001b[95m 0.1688  \u001b[0m | \u001b[95m 0.09769 \u001b[0m | \u001b[95m 0.2891  \u001b[0m | \u001b[95m 9.13    \u001b[0m | \u001b[95m 14.48   \u001b[0m | \u001b[95m 0.6029  \u001b[0m |\n",
      " Stopped after 57 iterations with train-auc = 0.971994 val-auc = 0.948385 ( diff = 0.023609 ) train-gini = 0.943987 val-gini = 0.896770\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.8968  \u001b[0m | \u001b[0m 0.9298  \u001b[0m | \u001b[0m 0.702   \u001b[0m | \u001b[0m 0.2911  \u001b[0m | \u001b[0m 0.9732  \u001b[0m | \u001b[0m 7.43    \u001b[0m | \u001b[0m 15.04   \u001b[0m | \u001b[0m 0.9643  \u001b[0m |\n",
      " Stopped after 1237 iterations with train-auc = 0.989440 val-auc = 0.951808 ( diff = 0.037633 ) train-gini = 0.978881 val-gini = 0.903616\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m 0.9036  \u001b[0m | \u001b[95m 0.4334  \u001b[0m | \u001b[95m 0.2631  \u001b[0m | \u001b[95m 0.01754 \u001b[0m | \u001b[95m 0.5662  \u001b[0m | \u001b[95m 11.16   \u001b[0m | \u001b[95m 15.43   \u001b[0m | \u001b[95m 0.7334  \u001b[0m |\n",
      " Stopped after 1891 iterations with train-auc = 0.979327 val-auc = 0.950437 ( diff = 0.028889 ) train-gini = 0.958653 val-gini = 0.900875\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.9009  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 2.659   \u001b[0m | \u001b[0m 11.96   \u001b[0m | \u001b[0m 15.67   \u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      " Stopped after 2139 iterations with train-auc = 0.979498 val-auc = 0.950233 ( diff = 0.029265 ) train-gini = 0.958997 val-gini = 0.900467\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.9005  \u001b[0m | \u001b[0m 0.4     \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 11.69   \u001b[0m | \u001b[0m 17.66   \u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      " Stopped after 2074 iterations with train-auc = 0.996571 val-auc = 0.952919 ( diff = 0.043652 ) train-gini = 0.993142 val-gini = 0.905838\n",
      "| \u001b[95m 12      \u001b[0m | \u001b[95m 0.9058  \u001b[0m | \u001b[95m 0.4789  \u001b[0m | \u001b[95m 3.128   \u001b[0m | \u001b[95m 0.01    \u001b[0m | \u001b[95m 0.4363  \u001b[0m | \u001b[95m 12.0    \u001b[0m | \u001b[95m 1.784   \u001b[0m | \u001b[95m 0.9824  \u001b[0m |\n",
      "=============================================================================================================\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "Final Results\n",
      "[{'target': 0.8982024, 'params': {'colsample_bytree': 0.47020733101804413, 'gamma': 6.08861747748796, 'learning_rate': 0.2583713033023701, 'max_delta_step': 1.5739576512021602, 'max_depth': 10.980149505155268, 'min_child_weight': 1.5247117297097157, 'subsample': 0.7647336318113128}}, {'target': 0.8939884, 'params': {'colsample_bytree': 0.5258176868161748, 'gamma': 9.253883343902084, 'learning_rate': 0.21138437948848363, 'max_delta_step': 1.0325111021101052, 'max_depth': 8.538144504949594, 'min_child_weight': 17.310023693467436, 'subsample': 0.44958602547465043}}, {'target': 0.8998784, 'params': {'colsample_bytree': 0.7527324151948561, 'gamma': 9.430636425855939, 'learning_rate': 0.0396557162958308, 'max_delta_step': 0.5508911995067711, 'max_depth': 5.547360554990086, 'min_child_weight': 16.792072642484953, 'subsample': 0.8192853244286045}}, {'target': 0.8988983999999998, 'params': {'colsample_bytree': 0.9232177049764042, 'gamma': 0.38651765185326525, 'learning_rate': 0.0908494176029192, 'max_delta_step': 0.16963309521539793, 'max_depth': 9.189797416245437, 'min_child_weight': 13.647605949215095, 'subsample': 0.4695740960695737}}, {'target': 0.8905988, 'params': {'colsample_bytree': 0.44231901898552445, 'gamma': 9.591777251913859, 'learning_rate': 0.4469691327222026, 'max_delta_step': 6.999821892119948, 'max_depth': 11.809703257213634, 'min_child_weight': 3.1358363441306913, 'subsample': 0.5971227727283275}}, {'target': 0.8742656, 'params': {'colsample_bytree': 0.41990991697179536, 'gamma': 8.209164176665437, 'learning_rate': 0.7981214120542841, 'max_delta_step': 2.1965208889600074, 'max_depth': 7.4780936308945325, 'min_child_weight': 1.1597868581233661, 'subsample': 0.5378794106217399}}, {'target': 0.9000495999999998, 'params': {'colsample_bytree': 0.5889984951058497, 'gamma': 0.16882376166422183, 'learning_rate': 0.09768882187762329, 'max_delta_step': 0.2891408096148129, 'max_depth': 9.129772363443386, 'min_child_weight': 14.478449707948197, 'subsample': 0.6029467797949207}}, {'target': 0.8967696000000001, 'params': {'colsample_bytree': 0.929828270116587, 'gamma': 0.7019846422485675, 'learning_rate': 0.2910940531859522, 'max_delta_step': 0.9731941941491817, 'max_depth': 7.429850154421195, 'min_child_weight': 15.043363661142594, 'subsample': 0.9642694846671914}}, {'target': 0.9036155999999997, 'params': {'colsample_bytree': 0.43338405882444747, 'gamma': 0.26305781883182894, 'learning_rate': 0.01753798046553043, 'max_delta_step': 0.5662249338471046, 'max_depth': 11.160659623490318, 'min_child_weight': 15.431645016072071, 'subsample': 0.7333526045435371}}, {'target': 0.9008748, 'params': {'colsample_bytree': 0.4, 'gamma': 0.001, 'learning_rate': 0.01, 'max_delta_step': 2.658995546733525, 'max_depth': 11.964830946215482, 'min_child_weight': 15.671320482388294, 'subsample': 0.4}}, {'target': 0.9004667999999998, 'params': {'colsample_bytree': 0.4, 'gamma': 0.001, 'learning_rate': 0.01, 'max_delta_step': 0.0, 'max_depth': 11.68685867453634, 'min_child_weight': 17.657652021676164, 'subsample': 0.4}}, {'target': 0.9058383999999999, 'params': {'colsample_bytree': 0.47892809228508343, 'gamma': 3.128150989095678, 'learning_rate': 0.01, 'max_delta_step': 0.43627018836029474, 'max_depth': 12.0, 'min_child_weight': 1.7837324932234644, 'subsample': 0.9823674200100508}}]\n"
     ]
    }
   ],
   "source": [
    "min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "cols = train.columns\n",
    "\n",
    "# 把数据的80%用来训练模型,20%做模型测试和评估,此处用到训练集-验证集二划分\n",
    "p = 0.8  # 设置训练数据比例,\n",
    "X_train = X[:int(len(X) * p), :]  # 前80%为训练集\n",
    "X_test = X[int(len(X) * p):, :]  # 后20%为测试集\n",
    "y_train = y[:int(len(y) * p)]  # 前80%为训练集\n",
    "y_test = y[int(len(y) * p):]  # 后20%为测试集\n",
    "\n",
    "\n",
    "# Comment out any parameter you don't want to test\n",
    "def XGB_CV(\n",
    "        learning_rate,\n",
    "        max_depth,\n",
    "        gamma,\n",
    "        min_child_weight,\n",
    "        max_delta_step,\n",
    "        subsample,\n",
    "        colsample_bytree\n",
    "):\n",
    "    global AUCbest\n",
    "    global ITERbest\n",
    "\n",
    "    # Define all XGboost parameters\n",
    "    paramt = {\n",
    "        'learning_rate': learning_rate,\n",
    "        'booster': 'gbtree',\n",
    "        'max_depth': int(max_depth),\n",
    "        'gamma': gamma,\n",
    "        'eta': 0.1,\n",
    "        'objective': 'binary:logistic',\n",
    "        'nthread': 4,\n",
    "        'silent': True,\n",
    "        'eval_metric': 'auc',\n",
    "        'subsample': max(min(subsample, 1), 0),\n",
    "        'colsample_bytree': max(min(colsample_bytree, 1), 0),\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'max_delta_step': int(max_delta_step),\n",
    "        'seed': 1001\n",
    "    }\n",
    "\n",
    "    folds = 5\n",
    "    cv_score = 0\n",
    "\n",
    "    print(\"\\n Search parameters (%d-fold validation):\\n %s\" % (folds, paramt), file=log_file)\n",
    "    log_file.flush()\n",
    "\n",
    "    xgbc = xgb.cv(\n",
    "        paramt,\n",
    "        dtrain,\n",
    "        num_boost_round=20000,\n",
    "        stratified=True,\n",
    "        nfold=folds,\n",
    "        #                    verbose_eval = 10,\n",
    "        early_stopping_rounds=100,\n",
    "        metrics='auc',\n",
    "        show_stdv=True\n",
    "    )\n",
    "\n",
    "    val_score = xgbc['test-auc-mean'].iloc[-1]\n",
    "    train_score = xgbc['train-auc-mean'].iloc[-1]\n",
    "    print(\n",
    "        ' Stopped after %d iterations with train-auc = %f val-auc = %f ( diff = %f ) train-gini = %f val-gini = %f' % (\n",
    "            len(xgbc), train_score, val_score, (train_score - val_score), (train_score * 2 - 1), (val_score * 2 - 1)))\n",
    "    if (val_score > AUCbest):\n",
    "        AUCbest = val_score\n",
    "        ITERbest = len(xgbc)\n",
    "\n",
    "    return (val_score * 2) - 1\n",
    "\n",
    "\n",
    "log_file = open('Porto-AUC-5fold-XGB-run-01-v1-full.log', 'a')\n",
    "AUCbest = -1.\n",
    "ITERbest = 0\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "XGB_BO = BayesianOptimization(XGB_CV, {\n",
    "    'learning_rate': (0.01, 1),\n",
    "    'max_depth': (5, 12),\n",
    "    'gamma': (0.001, 10.0),\n",
    "    'min_child_weight': (0, 20),\n",
    "    'max_delta_step': (0, 10),\n",
    "    'subsample': (0.4, 1.0),\n",
    "    'colsample_bytree': (0.4, 1.0)\n",
    "})\n",
    "\n",
    "# XGB_BO.explore({\n",
    "#     'learning_rate': [0.01, 0.03, 0.01, 0.03, 0.1, 0.3, 0.1, 0.3],\n",
    "#     'max_depth': [3, 8, 3, 8, 8, 3, 8, 3],\n",
    "#     'gamma': [0.5, 8, 0.2, 9, 0.5, 8, 0.2, 9],\n",
    "#     'min_child_weight': [0.2, 0.2, 0.2, 0.2, 12, 12, 12, 12],\n",
    "#     'max_delta_step': [1, 2, 2, 1, 2, 1, 1, 2],\n",
    "#     'subsample': [0.6, 0.8, 0.6, 0.8, 0.6, 0.8, 0.6, 0.8],\n",
    "#     'colsample_bytree': [0.6, 0.8, 0.6, 0.8, 0.6, 0.8, 0.6, 0.8],\n",
    "# })\n",
    "\n",
    "print('-' * 130)\n",
    "print('-' * 130, file=log_file)\n",
    "log_file.flush()\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    XGB_BO.maximize(init_points=2, n_iter=10, acq='ei', xi=0.0)\n",
    "\n",
    "print('-' * 130)\n",
    "print('Final Results')\n",
    "print(XGB_BO.res)\n",
    "# print('Maximum XGBOOST value: %f' % XGB_BO.res['max']['max_val'])\n",
    "# print('Best XGBOOST parameters: ', XGB_BO.res['max']['max_params'])\n",
    "# print('-' * 130, file=log_file)\n",
    "# print('Final Result:', file=log_file)\n",
    "# print('Maximum XGBOOST value: %f' % XGB_BO.res['max']['max_val'], file=log_file)\n",
    "# print('Best XGBOOST parameters: ', XGB_BO.res['max']['max_params'], file=log_file)\n",
    "# log_file.flush()\n",
    "# log_file.close()\n",
    "\n",
    "# history_df = pd.DataFrame(XGB_BO.res['all']['params'])\n",
    "# history_df2 = pd.DataFrame(XGB_BO.res['all']['values'])\n",
    "# history_df = pd.concat((history_df, history_df2), axis=1)\n",
    "# history_df.rename(columns={0: 'gini'}, inplace=True)\n",
    "# history_df['AUC'] = (history_df['gini'] + 1) / 2\n",
    "# history_df.to_csv('Porto-AUC-5fold-XGB-run-01-v1-grid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/base.py:213: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "# xgboost = XGBClassifier(\n",
    "#  learning_rate =0.01754,\n",
    "#  n_estimators=1237 ,\n",
    "#  max_depth=11,\n",
    "#     eta = 0.1,\n",
    "#  min_child_weight=15.43,\n",
    "#  gamma=0.2631,\n",
    "#  subsample=0.73348,\n",
    "#  colsample_bytree=0.4334,\n",
    "#  max_delta_step=0.5662,\n",
    "#  objective= 'binary:logistic',\n",
    "#  nthread=4,\n",
    "#  seed=1001)\n",
    "\n",
    "#         'learning_rate': learning_rate,\n",
    "#         'booster': 'gbtree',\n",
    "#         'max_depth': int(max_depth),\n",
    "#         'gamma': gamma,\n",
    "#         'eta': 0.1,\n",
    "#         'objective': 'binary:logistic',\n",
    "#         'nthread': 4,\n",
    "#         'silent': True,\n",
    "#         'eval_metric': 'auc',\n",
    "#         'subsample': max(min(subsample, 1), 0),\n",
    "#         'colsample_bytree': max(min(colsample_bytree, 1), 0),\n",
    "#         'min_child_weight': min_child_weight,\n",
    "#         'max_delta_step': int(max_delta_step),\n",
    "#         'seed': 1001\n",
    "\n",
    "xgboost = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " seed=27)\n",
    "\n",
    "\n",
    "xgboost_model = xgboost.fit(X_train, y_train.astype('int'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBOOST模型调参,测试样本0.1，训练集占0.9，模型效果为：\n",
    "# XGBOOST模型评估\n",
    "# 训练样本的准确率: 0.9718\n",
    "# average_precision: 0.6414\n",
    "# 查准率: 0.8901\n",
    "# 召回率: 0.6960\n",
    "# F1-Score: 0.7811\n",
    "# AUC-Score_predprob: 0.9639\n",
    "# 验证样本的准确率: 0.9710\n",
    "# 查准率: 0.8856\n",
    "# 召回率: 0.6887\n",
    "# F1-Score: 0.7748\n",
    "# AUC-Score: 0.9609\n",
    "# ('ks', 0.6817358342785097)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f= open('/Users/tuyu/000000毕业设计/Flight_Delay/Model/model/XGBOOST_Bayes_model.pickle','wb')\n",
    "pickle.dump(xgboost_model,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 看上面结果，迭代了12次，train-auc和val-auc，选取最好的，对应下面的参数，参数也有12组"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
